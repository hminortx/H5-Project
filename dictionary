#FYI, this was 100% generated by chatGPT after MANY iterations of me trying to get just what i wanted.



from Bio import SeqIO
from Bio import Entrez
import re
import time
import pandas as pd
import json  # Import the JSON module

# Set your email (required by NCBI)
Entrez.email = ""  # Replace with your email

# Load the FASTA file
fasta_file = ""  # Update with your filename

# Dictionary to store metadata
sequence_metadata = {}
processed_count = 0  # Initialize a counter for processed sequences

# Process each sequence in the FASTA file
for record in SeqIO.parse(fasta_file, "fasta"):
    # Extract Accession ID using regex
    match = re.match(r"^[A-Z0-9]+\.\d+", record.id)
    if match:
        accession_id = match.group(0)  # Extract only the valid accession ID
    else:
        print(f"Skipping invalid accession ID: {record.id}")
        continue  # Skip this sequence if no valid accession is found

    print(f"Processing Accession ID: {accession_id}")  # Debug statement
    metadata = {"Accession ID": accession_id}

    # Fetch the GenBank record using Entrez
    try:
        print(f"Fetching GenBank record for {accession_id}...")  # Debug statement
        handle = Entrez.efetch(db="nucleotide", id=accession_id, rettype="gb", retmode="text")
        genbank_record = handle.read()
        handle.close()
        print(f"Successfully fetched record for {accession_id}.")  # Debug statement

        # Parse the GenBank record for relevant features
        strain_match = re.search(r'/strain="([^"]+)"', genbank_record)
        serotype_match = re.search(r'/serotype="([^"]+)"', genbank_record)
        host_match = re.search(r'/host="([^"]+)"', genbank_record)
        location_match = re.search(r'/geo_loc_name="([^"]+)"', genbank_record)
        date_match = re.search(r'/collection_date="([^"]+)"', genbank_record)

        metadata["Strain"] = strain_match.group(1) if strain_match else "Unknown"
        metadata["Serotype"] = serotype_match.group(1) if serotype_match else "Unknown"
        metadata["Host"] = host_match.group(1) if host_match else "Unknown"
        metadata["Location"] = location_match.group(1) if location_match else "Unknown"
        metadata["Collection Date"] = date_match.group(1) if date_match else "Unknown"

        # Store the sequence
        metadata["Sequence"] = str(record.seq)

        # Add the metadata to the dictionary using the Accession ID as the key
        sequence_metadata[accession_id] = metadata
        
        # Increment the processed count
        processed_count += 1
        print(f"Processed {processed_count} sequences so far.\n")  # Print processed count
        
        # Sleep to avoid hitting rate limits
        time.sleep(1)

    except Exception as e:
        print(f"Could not fetch record for {accession_id}: {e}")

# Save the dictionary to a JSON file
with open("dictionary.json", "w") as json_file:
    json.dump(sequence_metadata, json_file, indent=4)
    print("Metadata has been saved to dictionary.json")

# Create a summary table
data = []

# Collecting data from the metadata dictionary
for metadata in sequence_metadata.values():
    host = metadata.get("Host", "Unknown")
    location = metadata.get("Location", "Unknown")
    year = metadata.get("Collection Date", "Unknown").split("-")[0]  # Extracting the year

    data.append({"Host": host, "Location": location, "Year": year})

# Create a DataFrame from the data
df = pd.DataFrame(data)

# Group by Host, Location, and Year, and count the number of sequences
summary = df.groupby(['Host', 'Location', 'Year']).size().reset_index(name='Total Sequences')

# Print the summary table
print("\nSummary Table:")
print(summary)

# Save the summary to a CSV file
summary.to_csv("summary_table.csv", index=False)
print("Summary table has been saved to summary_table.csv")
